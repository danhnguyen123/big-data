{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This notebook shows how to connect Jupyter notebooks to a Spark Cluster, read a local CSV and store it to Hadoop as partitioned parquet files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Connection to Spark Cluster\n",
    "\n",
    "To connect to the Spark cluster, create a SparkSession object with the following params:\n",
    "\n",
    "+ **appName:** application name displayed at the [Spark Master Web UI](http://localhost:8080/);\n",
    "+ **master:** Spark Master URL, same used by Spark Workers;\n",
    "+ **spark.executor.memory:** must be less than or equals to docker compose SPARK_WORKER_MEMORY config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/25 14:21:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Store Data\n",
    "We will now load data from a local CSV and store it to Hadoop partitioned by column.\n",
    "Afterward you can access Hadoop UI to explore the saved parquet files.\n",
    "Access Hadoop UI on 'http://localhost:9870' (Utilities -> Browse the files system )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "import time    \n",
    "epochNow = int(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from csv ./data/salesRecord.csv\n"
     ]
    }
   ],
   "source": [
    "#Iterate over all files until we find the sales file and then creates a Pandas dataframe.\n",
    "for path, subdirs, files in os.walk('./data/'):\n",
    "    for name in files:\n",
    "        if \"salesRecord\" in name:\n",
    "            csvName = name\n",
    "            csvPath = os.path.join(path, name)\n",
    "            print(\"Loading data from csv {}\".format(csvPath))\n",
    "            salesDfPandas = pandas.read_csv(csvPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/sql/pandas/conversion.py:327: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "#Create PySpark DataFrame from Pandas\n",
    "salesDfSpark=spark.createDataFrame(salesDfPandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales Dataframe created with schema : \n",
      "root\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Item_Type: string (nullable = true)\n",
      " |-- Sales_Channel: string (nullable = true)\n",
      " |-- Order_Priority: string (nullable = true)\n",
      " |-- Order_Date: string (nullable = true)\n",
      " |-- Order_ID: long (nullable = true)\n",
      " |-- Ship_Date: string (nullable = true)\n",
      " |-- Units_Sold: long (nullable = true)\n",
      " |-- Unit_Price: double (nullable = true)\n",
      " |-- Unit_Cost: double (nullable = true)\n",
      " |-- Total_Revenue: double (nullable = true)\n",
      " |-- Total_Cost: double (nullable = true)\n",
      " |-- Total_Profit: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Remove spaces in column names\n",
    "salesDfSpark = salesDfSpark.select([F.col(col).alias(col.replace(' ', '_')) for col in salesDfSpark.columns])\n",
    "print(\"Sales Dataframe created with schema : \")\n",
    "salesDfSpark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales Dataframe stored in Hadoop.\n"
     ]
    }
   ],
   "source": [
    "# Write Dataframe into HDFS\n",
    "# Repartition it by \"Country\" column before storing as parquet files in Hadoop\n",
    "salesDfSpark.write.option(\"header\",True) \\\n",
    "        .partitionBy(\"Country\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .parquet(\"hdfs://hadoop-namenode:9000/sales/{}_{}.parquet\".format(csvName,epochNow))\n",
    "print(\"Sales Dataframe stored in Hadoop.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales Dataframe read from Hadoop : \n",
      "+--------------------+---------------+-------------+--------------+----------+---------+----------+----------+----------+---------+-------------+----------+------------+-----------+\n",
      "|              Region|      Item_Type|Sales_Channel|Order_Priority|Order_Date| Order_ID| Ship_Date|Units_Sold|Unit_Price|Unit_Cost|Total_Revenue|Total_Cost|Total_Profit|    Country|\n",
      "+--------------------+---------------+-------------+--------------+----------+---------+----------+----------+----------+---------+-------------+----------+------------+-----------+\n",
      "|Middle East and N...|      Baby Food|       Online|             M| 2/21/2011|195833718|  4/7/2011|       404|    255.28|   159.42|    103133.12|  64405.68|    38727.44|    Bahrain|\n",
      "|Middle East and N...|         Fruits|       Online|             L|11/20/2015|282475936|11/28/2015|      9762|      9.33|     6.92|     91079.46|  67553.04|    23526.42|    Bahrain|\n",
      "|Middle East and N...|      Beverages|       Online|             L| 6/24/2015|953554761| 7/28/2015|      6899|     47.45|    31.79|    327357.55| 219319.21|   108038.34|    Bahrain|\n",
      "|Middle East and N...|  Personal Care|       Online|             C|  8/5/2014|502715766| 8/17/2014|      3621|     81.73|    56.67|    295944.33| 205202.07|    90742.26|    Bahrain|\n",
      "|Middle East and N...|         Fruits|       Online|             M| 3/14/2017|688344371| 4/28/2017|      5251|      9.33|     6.92|     48991.83|  36336.92|    12654.91|    Bahrain|\n",
      "|Middle East and N...|         Fruits|      Offline|             H| 9/28/2012|675548303| 11/6/2012|      8610|      9.33|     6.92|      80331.3|   59581.2|     20750.1|    Bahrain|\n",
      "|Middle East and N...|      Beverages|       Online|             C|  9/4/2010|133276879|10/17/2010|      8445|     47.45|    31.79|    400715.25| 268466.55|    132248.7|    Bahrain|\n",
      "|Middle East and N...|      Beverages|      Offline|             M| 2/28/2017|886628711| 3/31/2017|      1993|     47.45|    31.79|     94567.85|  63357.47|    31210.38|    Bahrain|\n",
      "|Central America a...|      Cosmetics|      Offline|             C|  2/7/2013|253407227| 2/15/2013|      7685|     437.2|   263.33|    3359882.0|2023691.05|  1336190.95|The Bahamas|\n",
      "|Central America a...|         Cereal|      Offline|             L|12/19/2013|450268065|  1/4/2014|      3181|     205.7|   117.11|     654331.7| 372526.91|   281804.79|The Bahamas|\n",
      "|Central America a...|      Baby Food|       Online|             C| 1/11/2013|300476777| 2/28/2013|      6610|    255.28|   159.42|    1687400.8| 1053766.2|    633634.6|The Bahamas|\n",
      "|Central America a...|         Snacks|       Online|             L| 4/24/2013|786519229|  6/7/2013|      7373|    152.58|    97.44|   1124972.34| 718425.12|   406547.22|The Bahamas|\n",
      "|Central America a...|         Cereal|      Offline|             M|11/27/2012|735968816| 12/6/2012|      8382|     205.7|   117.11|    1724177.4| 981616.02|   742561.38|The Bahamas|\n",
      "|Middle East and N...|           Meat|       Online|             C| 1/14/2011|972678697| 2/25/2011|      6096|    421.89|   364.69|   2571841.44|2223150.24|    348691.2|      Egypt|\n",
      "|Middle East and N...|      Baby Food|      Offline|             M|  9/8/2015|939787089|  9/9/2015|      2739|    255.28|   159.42|    699211.92| 436651.38|   262560.54|      Egypt|\n",
      "|Middle East and N...|           Meat|      Offline|             C| 6/17/2015|636879432|  7/3/2015|      5632|    421.89|   364.69|   2376084.48|2053934.08|    322150.4|      Egypt|\n",
      "|Middle East and N...|Office Supplies|       Online|             M| 8/23/2010|950427091| 9/14/2010|      1352|    651.21|   524.96|    880435.92| 709745.92|    170690.0|      Egypt|\n",
      "|Middle East and N...|         Snacks|      Offline|             C| 1/13/2017|596870315| 2/18/2017|      6045|    152.58|    97.44|     922346.1|  589024.8|    333321.3|      Egypt|\n",
      "|Middle East and N...|         Fruits|      Offline|             L| 9/13/2011|733563411| 9/20/2011|      6569|      9.33|     6.92|     61288.77|  45457.48|    15831.29|   Pakistan|\n",
      "|Middle East and N...|        Clothes|      Offline|             M| 6/28/2010|450849997| 7/21/2010|      5388|    109.28|    35.84|    588800.64| 193105.92|   395694.72|   Pakistan|\n",
      "+--------------------+---------------+-------------+--------------+----------+---------+----------+----------+----------+---------+-------------+----------+------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read from HDFS to confirm it was successfully stored\n",
    "df_load = spark.read.parquet(\"hdfs://hadoop-namenode:9000/sales/{}_{}.parquet\".format(csvName,epochNow))\n",
    "print(\"Sales Dataframe read from Hadoop : \")\n",
    "df_load.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
