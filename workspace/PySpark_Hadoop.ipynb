{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This notebook shows how to connect Jupyter notebooks to a Spark Cluster, read a local CSV and store it to Hadoop as partitioned parquet files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Connection to Spark Cluster\n",
    "\n",
    "To connect to the Spark cluster, create a SparkSession object with the following params:\n",
    "\n",
    "+ **appName:** application name displayed at the [Spark Master Web UI](http://localhost:8080/);\n",
    "+ **master:** Spark Master URL, same used by Spark Workers;\n",
    "+ **spark.executor.memory:** must be less than or equals to docker compose SPARK_WORKER_MEMORY config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/26 03:40:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"2G\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Store Data\n",
    "We will now load data from a local CSV and store it to Hadoop partitioned by column.\n",
    "Afterward you can access Hadoop UI to explore the saved parquet files.\n",
    "Access Hadoop UI on 'http://localhost:9870' (Utilities -> Browse the files system )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "import time    \n",
    "epochNow = int(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from csv ./data/salesRecord.csv\n"
     ]
    }
   ],
   "source": [
    "#Iterate over all files until we find the sales file and then creates a Pandas dataframe.\n",
    "for path, subdirs, files in os.walk('./data/'):\n",
    "    for name in files:\n",
    "        if \"salesRecord\" in name:\n",
    "            csvName = name\n",
    "            csvPath = os.path.join(path, name)\n",
    "            print(\"Loading data from csv {}\".format(csvPath))\n",
    "            salesDfPandas = pandas.read_csv(csvPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/sql/pandas/conversion.py:327: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "#Create PySpark DataFrame from Pandas\n",
    "salesDfSpark=spark.createDataFrame(salesDfPandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales Dataframe created with schema : \n",
      "root\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Item_Type: string (nullable = true)\n",
      " |-- Sales_Channel: string (nullable = true)\n",
      " |-- Order_Priority: string (nullable = true)\n",
      " |-- Order_Date: string (nullable = true)\n",
      " |-- Order_ID: long (nullable = true)\n",
      " |-- Ship_Date: string (nullable = true)\n",
      " |-- Units_Sold: long (nullable = true)\n",
      " |-- Unit_Price: double (nullable = true)\n",
      " |-- Unit_Cost: double (nullable = true)\n",
      " |-- Total_Revenue: double (nullable = true)\n",
      " |-- Total_Cost: double (nullable = true)\n",
      " |-- Total_Profit: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Remove spaces in column names\n",
    "salesDfSpark = salesDfSpark.select([F.col(col).alias(col.replace(' ', '_')) for col in salesDfSpark.columns])\n",
    "print(\"Sales Dataframe created with schema : \")\n",
    "salesDfSpark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales Dataframe stored in Hadoop.\n"
     ]
    }
   ],
   "source": [
    "# Write Dataframe into HDFS\n",
    "# Repartition it by \"Country\" column before storing as parquet files in Hadoop\n",
    "salesDfSpark.write.option(\"header\",True) \\\n",
    "        .partitionBy(\"Country\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .parquet(\"hdfs://hadoop-namenode:9000/sales/{}_{}.parquet\".format(csvName,epochNow))\n",
    "print(\"Sales Dataframe stored in Hadoop.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales Dataframe read from Hadoop : \n",
      "+--------------------+---------------+-------------+--------------+----------+---------+----------+----------+----------+---------+-------------+----------+------------+-------+\n",
      "|              Region|      Item_Type|Sales_Channel|Order_Priority|Order_Date| Order_ID| Ship_Date|Units_Sold|Unit_Price|Unit_Cost|Total_Revenue|Total_Cost|Total_Profit|Country|\n",
      "+--------------------+---------------+-------------+--------------+----------+---------+----------+----------+----------+---------+-------------+----------+------------+-------+\n",
      "|Middle East and N...|      Household|       Online|             H| 6/27/2012|927666509| 7/17/2012|      5990|    668.27|   502.54|    4002937.3| 3010214.6|    992722.7|Bahrain|\n",
      "|Middle East and N...|      Baby Food|       Online|             M| 2/21/2011|195833718|  4/7/2011|       404|    255.28|   159.42|    103133.12|  64405.68|    38727.44|Bahrain|\n",
      "|Middle East and N...|         Fruits|       Online|             L|11/20/2015|282475936|11/28/2015|      9762|      9.33|     6.92|     91079.46|  67553.04|    23526.42|Bahrain|\n",
      "|Middle East and N...|      Beverages|       Online|             L| 6/24/2015|953554761| 7/28/2015|      6899|     47.45|    31.79|    327357.55| 219319.21|   108038.34|Bahrain|\n",
      "|Middle East and N...|  Personal Care|       Online|             C|  8/5/2014|502715766| 8/17/2014|      3621|     81.73|    56.67|    295944.33| 205202.07|    90742.26|Bahrain|\n",
      "|Middle East and N...|         Fruits|       Online|             M| 3/14/2017|688344371| 4/28/2017|      5251|      9.33|     6.92|     48991.83|  36336.92|    12654.91|Bahrain|\n",
      "|Middle East and N...|         Fruits|      Offline|             H| 9/28/2012|675548303| 11/6/2012|      8610|      9.33|     6.92|      80331.3|   59581.2|     20750.1|Bahrain|\n",
      "|Middle East and N...|      Beverages|       Online|             C|  9/4/2010|133276879|10/17/2010|      8445|     47.45|    31.79|    400715.25| 268466.55|    132248.7|Bahrain|\n",
      "|Middle East and N...|      Beverages|      Offline|             M| 2/28/2017|886628711| 3/31/2017|      1993|     47.45|    31.79|     94567.85|  63357.47|    31210.38|Bahrain|\n",
      "|Central America a...|      Beverages|      Offline|             C|  1/9/2011|890695369| 2/23/2011|      5408|     47.45|    31.79|     256609.6| 171920.32|    84689.28|   Cuba|\n",
      "|Central America a...|        Clothes|      Offline|             H| 2/16/2011|466970717| 3/18/2011|      5867|    109.28|    35.84|    641145.76| 210273.28|   430872.48|   Cuba|\n",
      "|Central America a...|Office Supplies|      Offline|             H|  7/2/2017|256243503| 7/23/2017|      7002|    651.21|   524.96|   4559772.42|3675769.92|    884002.5|   Cuba|\n",
      "|Central America a...|      Household|      Offline|             L|  2/2/2013|576654183|  2/3/2013|      3642|    668.27|   502.54|   2433839.34|1830250.68|   603588.66|   Cuba|\n",
      "|Central America a...|           Meat|       Online|             M|10/17/2015|925504004| 12/6/2015|      6057|    421.89|   364.69|   2555387.73|2208927.33|    346460.4|   Cuba|\n",
      "|Central America a...|  Personal Care|       Online|             L| 8/23/2015|928647124| 8/30/2015|      6176|     81.73|    56.67|    504764.48| 349993.92|   154770.56|   Cuba|\n",
      "|Central America a...|      Cosmetics|      Offline|             C| 9/18/2015|925264966|10/18/2015|      5320|     437.2|   263.33|    2325904.0| 1400915.6|    924988.4|   Cuba|\n",
      "|Middle East and N...|  Personal Care|      Offline|             H| 8/26/2015|262749040| 8/30/2015|      2135|     81.73|    56.67|    174493.55| 120990.45|     53503.1| Israel|\n",
      "|Middle East and N...|  Personal Care|       Online|             M|12/10/2016|173571383| 1/11/2017|      2484|     81.73|    56.67|    203017.32| 140768.28|    62249.04| Israel|\n",
      "|Middle East and N...|           Meat|      Offline|             M| 12/1/2010|248093020|12/16/2010|      5093|    421.89|   364.69|   2148685.77|1857366.17|    291319.6| Israel|\n",
      "|Middle East and N...|      Beverages|      Offline|             C|12/15/2011|608414113|12/23/2011|      2111|     47.45|    31.79|    100166.95|  67108.69|    33058.26| Israel|\n",
      "+--------------------+---------------+-------------+--------------+----------+---------+----------+----------+----------+---------+-------------+----------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/26 03:45:27 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "22/11/26 03:45:27 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:716)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:152)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:258)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:168)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:203)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "# Read from HDFS to confirm it was successfully stored\n",
    "df_load = spark.read.parquet(\"hdfs://hadoop-namenode:9000/sales/{}_{}.parquet\".format(csvName,epochNow))\n",
    "print(\"Sales Dataframe read from Hadoop : \")\n",
    "df_load.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
