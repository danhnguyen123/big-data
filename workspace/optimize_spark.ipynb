{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark_context(app_name: str) -> SparkSession:\n",
    "    \"\"\"\n",
    "    Helper to manage the `SparkContext` and keep all of our\n",
    "    configuration params in one place. See below comments for details:\n",
    "        |_ https://github.com/bitnami/bitnami-docker-spark/issues/18#issuecomment-700628676\n",
    "        |_ https://github.com/leriel/pyspark-easy-start/blob/master/read_file.py\n",
    "    \"\"\"\n",
    "\n",
    "    conf = SparkConf()\n",
    "\n",
    "    conf.setAll(\n",
    "        [\n",
    "            (\"spark.master\", \"spark://spark-master:7077\"),\n",
    "            # (\"spark.driver.host\", \"172.23.0.2\"),\n",
    "            # (\"spark.submit.deployMode\", \"client\"),\n",
    "            # (\"spark.driver.bindAddress\", \"0.0.0.0\"),\n",
    "            (\"spark.app.name\", app_name),\n",
    "            (\"spark.dynamicAllocation.enabled\", \"false\"),\n",
    "            (\"spark.executor.memory\", \"4G\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/26 07:48:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = get_spark_context(\"Playgroud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technique 1: reduce data shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parkViolations = spark.read.option(\"header\", True).csv(\"hdfs://hadoop-namenode:9000/input/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "plateTypeCountDF = parkViolations.groupBy(\"Plate Type\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[Plate Type#19], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(Plate Type#19, 200), true, [id=#34]\n",
      "   +- *(1) HashAggregate(keys=[Plate Type#19], functions=[partial_count(1)])\n",
      "      +- FileScan csv [Plate Type#19] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[hdfs://hadoop-namenode:9000/input], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Plate Type:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "plateTypeCountDF.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "plateTypeCountDF.write.format(\"com.databricks.spark.csv\").option(\"header\", True).mode(\"overwrite\").save(\"hdfs://hadoop-namenode:9000/output/plate_type_count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "parkViolations = spark.read.option(\"header\", True).csv(\"hdfs://hadoop-namenode:9000/input/\")\n",
    "parkViolationsPlateTypeDF = parkViolations.repartition(87, \"Plate Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "Exchange hashpartitioning(Plate Type#200, 87), false, [id=#113]\n",
      "+- FileScan csv [Summons Number#197,Plate ID#198,Registration State#199,Plate Type#200,Issue Date#201,Violation Code#202,Vehicle Body Type#203,Vehicle Make#204,Issuing Agency#205,Street Code1#206,Street Code2#207,Street Code3#208,Vehicle Expiration Date#209,Violation Location#210,Violation Precinct#211,Issuer Precinct#212,Issuer Code#213,Issuer Command#214,Issuer Squad#215,Violation Time#216,Time First Observed#217,Violation County#218,Violation In Front Of Or Opposite#219,House Number#220,... 27 more fields] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[hdfs://hadoop-namenode:9000/input], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Summons Number:string,Plate ID:string,Registration State:string,Plate Type:string,Issue Da...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parkViolationsPlateTypeDF.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) HashAggregate(keys=[Plate Type#200], functions=[count(1)])\n",
      "+- *(1) HashAggregate(keys=[Plate Type#200], functions=[partial_count(1)])\n",
      "   +- Exchange hashpartitioning(Plate Type#200, 87), false, [id=#125]\n",
      "      +- FileScan csv [Plate Type#200] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[hdfs://hadoop-namenode:9000/input], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Plate Type:string>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "plateTypeCountDF = parkViolationsPlateTypeDF.groupBy(\"Plate Type\").count()\n",
    "plateTypeCountDF.explain() # check the execution plan, you will see the bottom 2 steps are for creating parkViolationsPlateTypeDF\n",
    "plateTypeCountDF.write.format(\"com.databricks.spark.csv\").option(\"header\", True).mode(\"overwrite\").save(\"/output/plate_type_count.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technique 2. Use caching, when necessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parkViolationsPlateTypeDF = parkViolations.repartition(87, \"Plate Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cachedDF = parkViolationsPlateTypeDF.select('Plate Type').cache() # we are caching only the required field of the  dataframe in memory to keep cache size small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "plateTypeCountDF = cachedDF.groupBy(\"Plate Type\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/26 07:50:14 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 8:=======================================>                   (6 + 3) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|Plate Type| count|\n",
      "+----------+------+\n",
      "|       AGR|   731|\n",
      "|       SOS|   246|\n",
      "|       WUG|     8|\n",
      "|       AYG|   333|\n",
      "|       SRN|  7327|\n",
      "|       VPL|    75|\n",
      "|       HOU|     1|\n",
      "|       TRL|  5696|\n",
      "|       JWV|     2|\n",
      "|       SNO|     2|\n",
      "|       MCD|   140|\n",
      "|       AMB|   196|\n",
      "|       MOT| 55722|\n",
      "|       OMV|   216|\n",
      "|       FPW|    60|\n",
      "|       LMC|    40|\n",
      "|       ATV|    25|\n",
      "|       HSM|    13|\n",
      "|       IRP|119641|\n",
      "|       PHS|  1952|\n",
      "+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "plateTypeCountDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "plateTypeCountDF.write.format(\"com.databricks.spark.csv\").option(\"header\", True).mode(\"overwrite\").save(\"hdfs://hadoop-namenode:9000/output/plate_type_count.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|Plate Type|\n",
      "+----------+\n",
      "|       AGR|\n",
      "|       SOS|\n",
      "|       WUG|\n",
      "|       AYG|\n",
      "|       SRN|\n",
      "|       VPL|\n",
      "|       HOU|\n",
      "|       TRL|\n",
      "|       JWV|\n",
      "|       SNO|\n",
      "|       MCD|\n",
      "|       AMB|\n",
      "|       MOT|\n",
      "|       OMV|\n",
      "|       FPW|\n",
      "|       LMC|\n",
      "|       ATV|\n",
      "|       HSM|\n",
      "|       IRP|\n",
      "|       PHS|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "plateTypeAvgDF = cachedDF.groupBy(\"Plate Type\").avg()\n",
    "plateTypeAvgDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "plateTypeAvgDF.write.format(\"com.databricks.spark.csv\").option(\"header\", True).mode(\"overwrite\").save(\"hdfs://hadoop-namenode:9000/output/plate_type_avg.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technique 3. Join strategies - broadcast join and bucketed joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1. Broadcast Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "parkViolations_2015 = spark.read.option(\"header\", True).csv(\"hdfs://hadoop-namenode:9000/input/2015.csv\")\n",
    "parkViolations_2016 = spark.read.option(\"header\", True).csv(\"hdfs://hadoop-namenode:9000/input/2016.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "parkViolations_2015 = parkViolations_2015.withColumnRenamed(\"Plate Type\", \"plateType\") # simple column rename for easier joins\n",
    "parkViolations_2016 = parkViolations_2016.withColumnRenamed(\"Plate Type\", \"plateType\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "parkViolations_2016_COM = parkViolations_2016.filter(parkViolations_2016.plateType == \"COM\")\n",
    "parkViolations_2015_COM = parkViolations_2015.filter(parkViolations_2015.plateType == \"COM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) Project [Summons Number#317, Issue Date#439]\n",
      "+- *(5) SortMergeJoin [plateType#793], [plateType#845], Inner\n",
      "   :- *(2) Sort [plateType#793 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(plateType#793, 200), true, [id=#272]\n",
      "   :     +- *(1) Project [Summons Number#317, Plate Type#320 AS plateType#793]\n",
      "   :        +- *(1) Filter (isnotnull(Plate Type#320) AND (Plate Type#320 = COM))\n",
      "   :           +- FileScan csv [Summons Number#317,Plate Type#320] Batched: false, DataFilters: [isnotnull(Plate Type#320), (Plate Type#320 = COM)], Format: CSV, Location: InMemoryFileIndex[hdfs://hadoop-namenode:9000/input/2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Plate Type), EqualTo(Plate Type,COM)], ReadSchema: struct<Summons Number:string,Plate Type:string>\n",
      "   +- *(4) Sort [plateType#845 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(plateType#845, 200), true, [id=#281]\n",
      "         +- *(3) Project [Plate Type#438 AS plateType#845, Issue Date#439]\n",
      "            +- *(3) Filter (isnotnull(Plate Type#438) AND (Plate Type#438 = COM))\n",
      "               +- FileScan csv [Plate Type#438,Issue Date#439] Batched: false, DataFilters: [isnotnull(Plate Type#438), (Plate Type#438 = COM)], Format: CSV, Location: InMemoryFileIndex[hdfs://hadoop-namenode:9000/input/2016.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Plate Type), EqualTo(Plate Type,COM)], ReadSchema: struct<Plate Type:string,Issue Date:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinDF = parkViolations_2015_COM.join(parkViolations_2016_COM, parkViolations_2015_COM.plateType ==  parkViolations_2016_COM.plateType, \"inner\").select(parkViolations_2015_COM[\"Summons Number\"], parkViolations_2016_COM[\"Issue Date\"])\n",
    "joinDF.explain() # you will see SortMergeJoin, with exchange for both dataframes, which means involves data shuffle of both dataframe\n",
    "# The below join will take a very long time with the given infrastructure, do not run, unless needed\n",
    "# joinDF.write.format(\"com.databricks.spark.csv\").option(\"header\", True).mode(\"overwrite\").save(\"/output/joined_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "parkViolations_2015_COM = parkViolations_2015.filter(parkViolations_2015.plateType == \"COM\").select(\"plateType\", \"Summons Number\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "parkViolations_2016_COM = parkViolations_2016.filter(parkViolations_2016.plateType == \"COM\").select(\"plateType\", \"Issue Date\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[plateType: string, Issue Date: string]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parkViolations_2015_COM.cache()\n",
    "parkViolations_2016_COM.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1183"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parkViolations_2015_COM.count() # will cause parkViolations_2015_COM to be cached\n",
    "parkViolations_2016_COM.count() # will cause parkViolations_2016_COM to be cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Project [Summons Number#317, Issue Date#439]\n",
      "+- *(2) BroadcastHashJoin [plateType#793], [plateType#845], Inner, BuildRight\n",
      "   :- *(2) Filter isnotnull(plateType#793)\n",
      "   :  +- InMemoryTableScan [plateType#793, Summons Number#317], [isnotnull(plateType#793)]\n",
      "   :        +- InMemoryRelation [plateType#793, Summons Number#317], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "   :              +- *(2) HashAggregate(keys=[plateType#793, Summons Number#317], functions=[])\n",
      "   :                 +- Exchange hashpartitioning(plateType#793, Summons Number#317, 200), true, [id=#370]\n",
      "   :                    +- *(1) HashAggregate(keys=[plateType#793, Summons Number#317], functions=[])\n",
      "   :                       +- *(1) Project [Plate Type#320 AS plateType#793, Summons Number#317]\n",
      "   :                          +- *(1) Filter (isnotnull(Plate Type#320) AND (Plate Type#320 = COM))\n",
      "   :                             +- FileScan csv [Summons Number#317,Plate Type#320] Batched: false, DataFilters: [isnotnull(Plate Type#320), (Plate Type#320 = COM)], Format: CSV, Location: InMemoryFileIndex[hdfs://hadoop-namenode:9000/input/2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Plate Type), EqualTo(Plate Type,COM)], ReadSchema: struct<Summons Number:string,Plate Type:string>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false])), [id=#464]\n",
      "      +- *(1) Filter isnotnull(plateType#845)\n",
      "         +- InMemoryTableScan [plateType#845, Issue Date#439], [isnotnull(plateType#845)]\n",
      "               +- InMemoryRelation [plateType#845, Issue Date#439], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                     +- *(2) HashAggregate(keys=[plateType#845, Issue Date#439], functions=[])\n",
      "                        +- Exchange hashpartitioning(plateType#845, Issue Date#439, 200), true, [id=#395]\n",
      "                           +- *(1) HashAggregate(keys=[plateType#845, Issue Date#439], functions=[])\n",
      "                              +- *(1) Project [Plate Type#438 AS plateType#845, Issue Date#439]\n",
      "                                 +- *(1) Filter (isnotnull(Plate Type#438) AND (Plate Type#438 = COM))\n",
      "                                    +- FileScan csv [Plate Type#438,Issue Date#439] Batched: false, DataFilters: [isnotnull(Plate Type#438), (Plate Type#438 = COM)], Format: CSV, Location: InMemoryFileIndex[hdfs://hadoop-namenode:9000/input/2016.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Plate Type), EqualTo(Plate Type,COM)], ReadSchema: struct<Plate Type:string,Issue Date:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinDF = parkViolations_2015_COM.join(parkViolations_2016_COM.hint(\"broadcast\"), parkViolations_2015_COM.plateType ==  parkViolations_2016_COM.plateType, \"inner\").select(parkViolations_2015_COM[\"Summons Number\"], parkViolations_2016_COM[\"Issue Date\"])\n",
    "joinDF.explain() # you will see BroadcastHashJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "joinDF.write.format(\"com.databricks.spark.csv\").option(\"header\", True).mode(\"overwrite\").save(\"hdfs://hadoop-namenode:9000/output/joined_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales Dataframe read from Hadoop : \n",
      "+--------------+----------+\n",
      "|           _c0|       _c1|\n",
      "+--------------+----------+\n",
      "|Summons Number|Issue Date|\n",
      "|    7779478117|01/08/2019|\n",
      "|    7779478117|09/02/2014|\n",
      "|    7779478117|04/28/2018|\n",
      "|    7779478117|10/16/2010|\n",
      "|    7779478117|10/31/2016|\n",
      "|    7779478117|03/17/2017|\n",
      "|    7779478117|04/06/2015|\n",
      "|    7779478117|01/30/2015|\n",
      "|    7779478117|05/24/2015|\n",
      "|    7779478117|02/24/2010|\n",
      "|    7779478117|02/27/2016|\n",
      "|    7779478117|03/01/2016|\n",
      "|    7779478117|07/08/2017|\n",
      "|    7779478117|07/22/2016|\n",
      "|    7779478117|10/29/2015|\n",
      "|    7779478117|04/26/2018|\n",
      "|    7779478117|10/27/2017|\n",
      "|    7779478117|10/30/2019|\n",
      "|    7779478117|06/08/2017|\n",
      "+--------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_load = spark.read.csv(\"hdfs://hadoop-namenode:9000/output/joined_df\")\n",
    "print(\"Sales Dataframe read from Hadoop : \")\n",
    "df_load.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2593756092"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_load.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2. Bucketed Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "parkViolations_2015 = spark.read.option(\"header\", True).csv(\"hdfs://hadoop-namenode:9000/input/2015.csv\")\n",
    "parkViolations_2016 = spark.read.option(\"header\", True).csv(\"hdfs://hadoop-namenode:9000/input/2016.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Summons_Number',\n",
       " 'Plate_ID',\n",
       " 'Registration_State',\n",
       " 'Plate_Type',\n",
       " 'Issue_Date',\n",
       " 'Violation_Code',\n",
       " 'Vehicle_Body_Type',\n",
       " 'Vehicle_Make',\n",
       " 'Issuing_Agency',\n",
       " 'Street_Code1',\n",
       " 'Street_Code2',\n",
       " 'Street_Code3',\n",
       " 'Vehicle_Expiration_Date',\n",
       " 'Violation_Location',\n",
       " 'Violation_Precinct',\n",
       " 'Issuer_Precinct',\n",
       " 'Issuer_Code',\n",
       " 'Issuer_Command',\n",
       " 'Issuer_Squad',\n",
       " 'Violation_Time',\n",
       " 'Time_First_Observed',\n",
       " 'Violation_County',\n",
       " 'Violation_In_Front_Of_Or_Opposite',\n",
       " 'House_Number',\n",
       " 'Street_Name',\n",
       " 'Intersecting_Street',\n",
       " 'Date_First_Observed',\n",
       " 'Law_Section',\n",
       " 'Sub_Division',\n",
       " 'Violation_Legal_Code',\n",
       " 'Days_Parking_In_Effect____',\n",
       " 'From_Hours_In_Effect',\n",
       " 'To_Hours_In_Effect',\n",
       " 'Vehicle_Color',\n",
       " 'Unregistered_Vehicle?',\n",
       " 'Vehicle_Year',\n",
       " 'Meter_Number',\n",
       " 'Feet_From_Curb',\n",
       " 'Violation_Post_Code',\n",
       " 'Violation_Description',\n",
       " 'No_Standing_or_Stopping_Violation',\n",
       " 'Hydrant_Violation',\n",
       " 'Double_Parking_Violation',\n",
       " 'Latitude',\n",
       " 'Longitude',\n",
       " 'Community_Board',\n",
       " 'Community_Council_',\n",
       " 'Census_Tract',\n",
       " 'BIN',\n",
       " 'BBL',\n",
       " 'NTA']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_column_name_list= list(map(lambda x: x.replace(\" \", \"_\"), parkViolations_2015.columns))\n",
    "new_column_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "parkViolations_2015 = parkViolations_2015.toDF(*new_column_name_list)\n",
    "parkViolations_2016 = parkViolations_2016.toDF(*new_column_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 87:====================================================>   (16 + 1) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|Plate_Type|Vehicle_Year|\n",
      "+----------+------------+\n",
      "|       COM|        1997|\n",
      "|       PAS|        1988|\n",
      "|       THC|        2011|\n",
      "|       ITP|        2006|\n",
      "|       TRC|        2007|\n",
      "|       OMR|        1995|\n",
      "|       OMR|        1997|\n",
      "|       PAS|        2036|\n",
      "|       AMB|        2012|\n",
      "|       TRA|        1991|\n",
      "|       NYC|        2005|\n",
      "|       COM|        2027|\n",
      "|       LMB|        2012|\n",
      "|       SOS|        2004|\n",
      "|       FAR|        2015|\n",
      "|       COM|        1992|\n",
      "|       OMR|        2001|\n",
      "|       SPO|        2004|\n",
      "|       CMH|        2005|\n",
      "|       OMS|        2056|\n",
      "+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parkViolations_2016.groupBy('Plate_Type','Vehicle_Year').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parkViolations_2015 = parkViolations_2015.filter(parkViolations_2015.Plate_Type == \"COM\").filter(parkViolations_2015.Vehicle_Year == \"1997\")\n",
    "parkViolations_2016 = parkViolations_2016.filter(parkViolations_2016.Plate_Type == \"COM\").filter(parkViolations_2016.Vehicle_Year == \"1997\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    " # we do this so that Spark does not auto optimize for broadcast join, setting to -1 means disable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parkViolations_2015.write.mode(\"overwrite\").bucketBy(400, \"Vehicle_Year\", \"plate_type\").saveAsTable(\"parkViolations_bkt_2015\")\n",
    "parkViolations_2016.write.mode(\"overwrite\").bucketBy(400, \"Vehicle_Year\", \"plate_type\").saveAsTable(\"parkViolations_bkt_2016\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "parkViolations_2015_tbl = spark.read.table(\"parkViolations_bkt_2015\")\n",
    "parkViolations_2016_tbl = spark.read.table(\"parkViolations_bkt_2016\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinDF = parkViolations_2015_tbl.join(parkViolations_2016_tbl, (parkViolations_2015_tbl.Plate_Type ==  parkViolations_2016_tbl.Plate_Type) & (parkViolations_2015_tbl.Vehicle_Year ==  parkViolations_2016_tbl.Vehicle_Year) , \"inner\").select(parkViolations_2015_tbl[\"Summons_Number\"], parkViolations_2016_tbl[\"Issue_Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) Project [Summons_Number#4110, Issue_Date#4216]\n",
      "+- *(3) SortMergeJoin [Vehicle_Year#4145, Plate_Type#4113], [Vehicle_Year#4247, Plate_Type#4215], Inner\n",
      "   :- *(1) Sort [Vehicle_Year#4145 ASC NULLS FIRST, Plate_Type#4113 ASC NULLS FIRST], false, 0\n",
      "   :  +- *(1) Project [Summons_Number#4110, Plate_Type#4113, Vehicle_Year#4145]\n",
      "   :     +- *(1) Filter (isnotnull(Plate_Type#4113) AND isnotnull(Vehicle_Year#4145))\n",
      "   :        +- *(1) ColumnarToRow\n",
      "   :           +- FileScan parquet default.parkviolations_bkt_2015[Summons_Number#4110,Plate_Type#4113,Vehicle_Year#4145] Batched: true, DataFilters: [isnotnull(Plate_Type#4113), isnotnull(Vehicle_Year#4145)], Format: Parquet, Location: InMemoryFileIndex[file:/workspace/spark-warehouse/parkviolations_bkt_2015], PartitionFilters: [], PushedFilters: [IsNotNull(Plate_Type), IsNotNull(Vehicle_Year)], ReadSchema: struct<Summons_Number:string,Plate_Type:string,Vehicle_Year:string>, SelectedBucketsCount: 400 out of 400\n",
      "   +- *(2) Sort [Vehicle_Year#4247 ASC NULLS FIRST, Plate_Type#4215 ASC NULLS FIRST], false, 0\n",
      "      +- *(2) Project [Plate_Type#4215, Issue_Date#4216, Vehicle_Year#4247]\n",
      "         +- *(2) Filter (isnotnull(Plate_Type#4215) AND isnotnull(Vehicle_Year#4247))\n",
      "            +- *(2) ColumnarToRow\n",
      "               +- FileScan parquet default.parkviolations_bkt_2016[Plate_Type#4215,Issue_Date#4216,Vehicle_Year#4247] Batched: true, DataFilters: [isnotnull(Plate_Type#4215), isnotnull(Vehicle_Year#4247)], Format: Parquet, Location: InMemoryFileIndex[file:/workspace/spark-warehouse/parkviolations_bkt_2016], PartitionFilters: [], PushedFilters: [IsNotNull(Plate_Type), IsNotNull(Vehicle_Year)], ReadSchema: struct<Plate_Type:string,Issue_Date:string,Vehicle_Year:string>, SelectedBucketsCount: 400 out of 400\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinDF.explain() # you will see SortMergeJoin, but no exchange, which means no data shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# The below join will take a while, approx 30min\n",
    "joinDF.write.format(\"com.databricks.spark.csv\").option(\"header\", True).mode(\"overwrite\").save(\"hdfs://hadoop-namenode:9000/output/bkt_joined_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales Dataframe read from Hadoop : \n",
      "+--------------+----------+\n",
      "|Summons_Number|Issue_Date|\n",
      "+--------------+----------+\n",
      "+--------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/26 09:12:05 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "22/11/26 09:12:05 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:716)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:152)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:258)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:168)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:203)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "joinDF_bk_load = spark.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").load(\"hdfs://hadoop-namenode:9000/output/bkt_joined_df.csv\")\n",
    "print(\"Sales Dataframe read from Hadoop : \")\n",
    "joinDF_bk_load.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
